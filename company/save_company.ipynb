{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T09:24:38.942884147Z",
     "start_time": "2025-04-04T09:24:38.901203637Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import helpers\n",
    "import hashlib\n",
    "from utils.esutils import es_client\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 字段映射字典：中文字段名 -> 英文字段名\n",
    "FIELD_MAPPING = {\n",
    "    \"企业名称\": \"company_name\",\n",
    "    \"英文名称\": \"english_name\",\n",
    "    \"统一社会信用代码\": \"unified_social_credit_code\",\n",
    "    \"企业类型\": \"company_type\",\n",
    "    \"经营状态\": \"business_status\",\n",
    "    \"成立日期\": \"establishment_date\",\n",
    "    \"核准日期\": \"approval_date\",\n",
    "    \"法定代表人\": \"legal_representative\",\n",
    "    \"注册资本\": \"registered_capital\",\n",
    "    \"实缴资本\": \"paid_in_capital\",\n",
    "    \"参保人数\": \"insured_employees\",\n",
    "    \"公司规模\": \"company_size\",\n",
    "    \"经营范围\": \"business_scope\",\n",
    "    \"注册地址\": \"registered_address\",\n",
    "    \"营业期限\": \"business_term\",\n",
    "    \"来源\": \"source\",\n",
    "    \"纳税人识别号\": \"taxpayer_identification_number\",\n",
    "    \"工商注册号\": \"business_registration_number\",\n",
    "    \"组织机构代码\": \"organization_code\",\n",
    "    \"联系电话\": \"contact_phone\",\n",
    "    \"邮箱\": \"email\",\n",
    "    \"纳税人资质\": \"taxpayer_qualification\",\n",
    "    \"曾用名\": \"former_name\",\n",
    "    \"所属省份\": \"province\",\n",
    "    \"所属城市\": \"city\",\n",
    "    \"所属区县\": \"district\",\n",
    "    \"网站链接\": \"website\",\n",
    "    \"网址\": \"website\",  # 兼容旧字段名\n",
    "    \"所属行业\": \"industry\",\n",
    "    \"一级行业分类\": \"primary_industry\",\n",
    "    \"二级行业分类\": \"secondary_industry\",\n",
    "    \"三级行业分类\": \"tertiary_industry\",\n",
    "    \"登记机关\": \"registration_authority\",\n",
    "    \"经度\": \"longitude\",  # 临时字段，用于生成 location\n",
    "    \"纬度\": \"latitude\"     # 临时字段，用于生成 location\n",
    "}\n",
    "\n",
    "def get_unique_id(province, city, district, address, company_name):\n",
    "    \"\"\"\n",
    "    生成唯一 ID，基于省份、城市、区域、注册地址和公司名。\n",
    "    \n",
    "    Args:\n",
    "        province (str): 省份。\n",
    "        city (str): 城市。\n",
    "        district (str): 区域。\n",
    "        address (str): 注册地址。\n",
    "        company_name (str): 公司名。\n",
    "    \n",
    "    Returns:\n",
    "        str: 唯一 ID。\n",
    "    \"\"\"\n",
    "    # 处理空值和值为 \"-\"\n",
    "    province = province if province and province != \"-\" else \"unknown_province\"\n",
    "    city = city if city and city != \"-\" else \"unknown_city\"\n",
    "    district = district if district and district != \"-\" else \"unknown_district\"\n",
    "    address = address if address and address != \"-\" else \"unknown_address\"\n",
    "    company_name = company_name if company_name and company_name != \"-\" else \"unknown_company\"\n",
    "    \n",
    "    # 拼接字段值\n",
    "    combined = f\"{province}{city}{district}{address}{company_name}\"\n",
    "    \n",
    "    # 使用 MD5 生成唯一 ID\n",
    "    return hashlib.md5(combined.encode('utf-8')).hexdigest()\n",
    "\n",
    "def bulk_with_retry(es_client, actions, retries=3):\n",
    "    \"\"\"\n",
    "    带重试机制的批量写入。\n",
    "    \n",
    "    Args:\n",
    "        es_client: Elasticsearch 客户端。\n",
    "        actions (list): 批量操作列表。\n",
    "        retries (int): 重试次数。\n",
    "    \n",
    "    Returns:\n",
    "        bool: 是否成功。\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            helpers.bulk(es_client, actions)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"批量写入失败，尝试 {attempt + 1}/{retries}: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                raise e\n",
    "    return False\n",
    "\n",
    "def import_to_es(df, index_name=\"enterprise_info\", batch_size=5000, retries=1):\n",
    "    \"\"\"\n",
    "    将 DataFrame 数据批量写入 Elasticsearch。\n",
    "    如果字段值为 \"-\"，则不写入该字段。\n",
    "    将经度和纬度合并为 location 字段（geo_point 类型）。\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): 包含企业信息的 DataFrame，字段名为中文。\n",
    "        index_name (str): 目标索引名。\n",
    "        batch_size (int): 批量写入的大小。\n",
    "        retries (int): 批量写入失败时的重试次数。\n",
    "    \"\"\"\n",
    "\n",
    "    actions = []\n",
    "    processed_count = 0\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # 遍历 DataFrame 的每一行\n",
    "    for _, row in df.iterrows():\n",
    "        # 构建文档内容，将中文字段名映射为英文\n",
    "        info = {}\n",
    "        longitude = None\n",
    "        latitude = None\n",
    "\n",
    "        for cn_field, en_field in FIELD_MAPPING.items():\n",
    "            value = row.get(cn_field)\n",
    "            # 跳过 NaN 和值为 \"-\" 的字段\n",
    "            if pd.isna(value) or value == \"-\":\n",
    "                continue\n",
    "            # 临时存储 longitude 和 latitude\n",
    "            if en_field == \"longitude\":\n",
    "                longitude = value\n",
    "                continue\n",
    "            if en_field == \"latitude\":\n",
    "                latitude = value\n",
    "                continue\n",
    "            info[en_field] = value\n",
    "\n",
    "        # 如果 longitude 和 latitude 都存在，生成 location 字段\n",
    "        if longitude is not None and latitude is not None:\n",
    "            try:\n",
    "                info[\"location\"] = {\n",
    "                    \"lat\": float(latitude),\n",
    "                    \"lon\": float(longitude)\n",
    "                }\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"无法转换经纬度为 location 字段: longitude={longitude}, latitude={latitude}\")\n",
    "\n",
    "        # 生成唯一 ID\n",
    "        unique_id = get_unique_id(\n",
    "            info.get(\"province\"),\n",
    "            info.get(\"city\"),\n",
    "            info.get(\"district\"),\n",
    "            info.get(\"registered_address\"),\n",
    "            info.get(\"company_name\")\n",
    "        )\n",
    "\n",
    "        # 构建 action\n",
    "        action = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": unique_id,\n",
    "            \"_source\": info\n",
    "        }\n",
    "        actions.append(action)\n",
    "        processed_count += 1\n",
    "\n",
    "        # 批量写入\n",
    "        if len(actions) >= batch_size:\n",
    "            try:\n",
    "                bulk_with_retry(es_client, actions, retries)\n",
    "                actions.clear()\n",
    "                print(f\"已处理 {processed_count}/{total_rows} 条数据\")\n",
    "            except Exception as e:\n",
    "                print(f\"批量写入失败: {e}\")\n",
    "                print(actions)\n",
    "                raise e\n",
    "\n",
    "    # 处理剩余的数据\n",
    "    if actions:\n",
    "        try:\n",
    "            bulk_with_retry(es_client, actions, retries)\n",
    "            print(f\"已处理 {processed_count}/{total_rows} 条数据（最后一批）\")\n",
    "        except Exception as e:\n",
    "            print(f\"批量写入失败（最后一批）: {e}\")\n",
    "            raise e\n",
    "\n",
    "    print(f\"总共处理 {processed_count}/{total_rows} 条数据，写入完成！\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T09:24:46.967979435Z",
     "start_time": "2025-04-04T09:24:46.959014359Z"
    }
   },
   "id": "97b55cfd37036829",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 10000/500002 条数据\n",
      "已处理 20000/500002 条数据\n",
      "已处理 30000/500002 条数据\n",
      "已处理 40000/500002 条数据\n",
      "已处理 50000/500002 条数据\n",
      "已处理 60000/500002 条数据\n",
      "已处理 70000/500002 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "BulkIndexError",
     "evalue": "1 document(s) failed to index.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBulkIndexError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m company \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(source_dir):\n\u001B[1;32m      4\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(source_dir \u001B[38;5;241m+\u001B[39m company)\n\u001B[0;32m----> 5\u001B[0m     import_to_es(df, index_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menterprise_info\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m完成入库\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcompany\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m     shutil\u001B[38;5;241m.\u001B[39mmove(source_dir \u001B[38;5;241m+\u001B[39m company, aim_dir \u001B[38;5;241m+\u001B[39m company)\n",
      "Cell \u001B[0;32mIn[43], line 165\u001B[0m, in \u001B[0;36mimport_to_es\u001B[0;34m(df, index_name, batch_size, retries)\u001B[0m\n\u001B[1;32m    163\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m批量写入失败: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    164\u001B[0m             \u001B[38;5;28mprint\u001B[39m(actions)\n\u001B[0;32m--> 165\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m \u001B[38;5;66;03m# 处理剩余的数据\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m actions:\n",
      "Cell \u001B[0;32mIn[43], line 159\u001B[0m, in \u001B[0;36mimport_to_es\u001B[0;34m(df, index_name, batch_size, retries)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(actions) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m batch_size:\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 159\u001B[0m         bulk_with_retry(es_client, actions, retries)\n\u001B[1;32m    160\u001B[0m         actions\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m已处理 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprocessed_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_rows\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 条数据\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[43], line 86\u001B[0m, in \u001B[0;36mbulk_with_retry\u001B[0;34m(es_client, actions, retries)\u001B[0m\n\u001B[1;32m     84\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m批量写入失败，尝试 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattempt\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretries\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m attempt \u001B[38;5;241m==\u001B[39m retries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[43], line 81\u001B[0m, in \u001B[0;36mbulk_with_retry\u001B[0;34m(es_client, actions, retries)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(retries):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m         helpers\u001B[38;5;241m.\u001B[39mbulk(es_client, actions)\n\u001B[1;32m     82\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/fastApiProject/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:521\u001B[0m, in \u001B[0;36mbulk\u001B[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# make streaming_bulk yield successful results so we can count them\u001B[39;00m\n\u001B[1;32m    520\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myield_ok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 521\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ok, item \u001B[38;5;129;01min\u001B[39;00m streaming_bulk(\n\u001B[1;32m    522\u001B[0m     client, actions, ignore_status\u001B[38;5;241m=\u001B[39mignore_status, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    523\u001B[0m ):\n\u001B[1;32m    524\u001B[0m     \u001B[38;5;66;03m# go through request-response pairs and detect failures\u001B[39;00m\n\u001B[1;32m    525\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ok:\n\u001B[1;32m    526\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stats_only:\n",
      "File \u001B[0;32m~/miniconda3/envs/fastApiProject/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:436\u001B[0m, in \u001B[0;36mstreaming_bulk\u001B[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001B[0m\n\u001B[1;32m    433\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;28mmin\u001B[39m(max_backoff, initial_backoff \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m (attempt \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)))\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 436\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data, (ok, info) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    437\u001B[0m         bulk_data,\n\u001B[1;32m    438\u001B[0m         _process_bulk_chunk(\n\u001B[1;32m    439\u001B[0m             client,\n\u001B[1;32m    440\u001B[0m             bulk_actions,\n\u001B[1;32m    441\u001B[0m             bulk_data,\n\u001B[1;32m    442\u001B[0m             raise_on_exception,\n\u001B[1;32m    443\u001B[0m             raise_on_error,\n\u001B[1;32m    444\u001B[0m             ignore_status,\n\u001B[1;32m    445\u001B[0m             \u001B[38;5;241m*\u001B[39margs,\n\u001B[1;32m    446\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    447\u001B[0m         ),\n\u001B[1;32m    448\u001B[0m     ):\n\u001B[1;32m    449\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ok:\n\u001B[1;32m    450\u001B[0m             action, info \u001B[38;5;241m=\u001B[39m info\u001B[38;5;241m.\u001B[39mpopitem()\n",
      "File \u001B[0;32m~/miniconda3/envs/fastApiProject/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:355\u001B[0m, in \u001B[0;36m_process_bulk_chunk\u001B[0;34m(client, bulk_actions, bulk_data, raise_on_exception, raise_on_error, ignore_status, *args, **kwargs)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    349\u001B[0m     gen \u001B[38;5;241m=\u001B[39m _process_bulk_chunk_success(\n\u001B[1;32m    350\u001B[0m         resp\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mbody,\n\u001B[1;32m    351\u001B[0m         bulk_data\u001B[38;5;241m=\u001B[39mbulk_data,\n\u001B[1;32m    352\u001B[0m         ignore_status\u001B[38;5;241m=\u001B[39mignore_status,\n\u001B[1;32m    353\u001B[0m         raise_on_error\u001B[38;5;241m=\u001B[39mraise_on_error,\n\u001B[1;32m    354\u001B[0m     )\n\u001B[0;32m--> 355\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m gen\n",
      "File \u001B[0;32m~/miniconda3/envs/fastApiProject/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:274\u001B[0m, in \u001B[0;36m_process_bulk_chunk_success\u001B[0;34m(resp, bulk_data, ignore_status, raise_on_error)\u001B[0m\n\u001B[1;32m    271\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m ok, {op_type: item}\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m errors:\n\u001B[0;32m--> 274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BulkIndexError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(errors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m document(s) failed to index.\u001B[39m\u001B[38;5;124m\"\u001B[39m, errors)\n",
      "\u001B[0;31mBulkIndexError\u001B[0m: 1 document(s) failed to index."
     ]
    }
   ],
   "source": [
    "source_dir = '待入库/'\n",
    "aim_dir = \"已入库/\"\n",
    "for company in os.listdir(source_dir):\n",
    "    df = pd.read_excel(source_dir + company)\n",
    "    import_to_es(df, index_name=\"enterprise_info\", batch_size=10000, retries=1)\n",
    "    print(f\"完成入库{company}\")\n",
    "    shutil.move(source_dir + company, aim_dir + company)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T12:02:07.070937343Z",
     "start_time": "2025-04-04T12:00:27.014459869Z"
    }
   },
   "id": "68b4c4a1520ca5a1",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8e3ac1a06ce9704"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
